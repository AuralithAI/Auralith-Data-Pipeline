name: Data Pipeline

on:
  schedule:
    - cron: "0 0 * * 0"  # Weekly on Sunday
  workflow_dispatch:
    inputs:
      dataset:
        description: "Dataset to collect"
        required: true
        type: choice
        options:
          - wikipedia
          - arxiv
          - openwebtext
          - dolly
      max_samples:
        description: "Maximum samples to collect"
        required: false
        default: "100000"
        type: string
      upload:
        description: "Upload to HuggingFace Hub"
        required: false
        default: true
        type: boolean

jobs:
  collect:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[all]"
      
      - name: Collect dataset
        run: |
          python -m auralith_pipeline.cli collect \
            --dataset ${{ github.event.inputs.dataset || 'wikipedia' }} \
            --max-samples ${{ github.event.inputs.max_samples || '100000' }} \
            --output ./data/shards
      
      - name: Upload to HuggingFace Hub
        if: ${{ github.event.inputs.upload == 'true' }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -m auralith_pipeline.cli upload \
            --source ./data/shards \
            --dest hf://AuralithAI/training-data
      
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: shards-${{ github.event.inputs.dataset || 'wikipedia' }}
          path: ./data/shards/
          retention-days: 7
